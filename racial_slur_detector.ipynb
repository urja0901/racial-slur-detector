{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U \"mlfoundry<0.5.0\""
      ],
      "metadata": {
        "id": "gc_g9W6CzLk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mlfoundry login --tracking_uri \"https://app.truefoundry.com\""
      ],
      "metadata": {
        "id": "3IVJh9zMzXwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea69d652-e296-4594-d67a-9979f79654f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[1mMlFoundry CLI\u001b[0m\n",
            "Please get your API key from https://app.truefoundry.com/settings\n",
            "Paste your API key and hit enter:\n",
            "Writing API key at /root/.mlfoundry/credentials.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MicP4ZkBtUMx",
        "outputId": "06e3da58-61be-4f84-d095-c8bef1bf9e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import pickle\n",
        "import pandas as pd \n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "train_data_path = \"train.csv\"\n",
        "test_data_path = \"test.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(train_data_path)\n",
        "test=pd.read_csv(test_data_path)\n",
        "text=df['tweet'].values.tolist()\n",
        "text_test=test['tweet'].values.tolist()\n",
        "text+=text_test"
      ],
      "metadata": {
        "id": "hjC6_uqvvXiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "stopword.remove('not')\n",
        "for index,text_ in enumerate(text):\n",
        "    text_=re.sub(r'@[\\w]*','',text_) #Removing Twitter Handles (@user)\n",
        "    text_=re.sub(r'http/S+','',text_) #Removing urls from text \n",
        "    text_=re.sub(r'[^A-Za-z#]',' ',text_) #Removing Punctuations, Numbers, and Special Characters\n",
        "    text_=\" \".join(i.lower() for i in text_.split() if i.lower() not in stopword) #Removing stopword\n",
        "    text[index]=text_\n"
      ],
      "metadata": {
        "id": "VwQ59NFwvg_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming the word\n",
        "pt=PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()\n",
        "for index,text_ in enumerate(text):\n",
        "    text_=\" \".join(pt.stem(i) for i in text_.split())\n",
        "    text_=\" \".join(wordnet.lemmatize(i) for i in text_.split())  \n",
        "    text[index]=text_\n",
        "\n",
        "df['preprocess_tweet']=text[:len(df)]\n",
        "df['length_tweet']=df['preprocess_tweet'].str.len()\n",
        "test['preprocess_tweet']=text[len(df):]"
      ],
      "metadata": {
        "id": "X7pZbdNcvrke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=df.copy()\n",
        "train.drop(columns=['id','tweet','preprocess_tweet'],inplace=True)"
      ],
      "metadata": {
        "id": "mk5ooW15vujN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgAmaXeqvyRN",
        "outputId": "641a3aba-94d0-4447-e83d-0f136edf2ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((31962, 2), (17197, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow=CountVectorizer(max_features=1000)\n",
        "bow.fit(df['preprocess_tweet'])\n",
        "bow_df=bow.transform(df['preprocess_tweet']).toarray()\n",
        "print('feature name==',bow.get_feature_names_out()[:10])\n",
        "print('number of uniqe words',bow_df.shape[1])\n",
        "print('shape',bow_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff1aOuswxaNe",
        "outputId": "553013e8-f6d6-4e16-c0e6-e7887d2e6955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature name== ['abl' 'absolut' 'accept' 'account' 'act' 'action' 'actor' 'actual' 'ad'\n",
            " 'adapt']\n",
            "number of uniqe words 1000\n",
            "shape (31962, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_train=pd.DataFrame(bow_df)\n",
        "bow_train['length_tweet']=df['length_tweet']\n",
        "bow_train['label']=df['label']\n",
        "\n",
        "major_class_0,major_class_1=bow_train.label.value_counts()\n",
        "df_major=bow_train[bow_train['label']==0]\n",
        "df_minor=bow_train[bow_train['label']==1]\n",
        "df_minor_upsampled = resample(df_minor, \n",
        "                                replace=True,     # sample with replacement\n",
        "                                n_samples=major_class_0)\n",
        "df_bow_upsampled = pd.concat([df_major, df_minor_upsampled])\n",
        "\n",
        "x=df_bow_upsampled.iloc[:,0:-1]\n",
        "y=df_bow_upsampled['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdPDIs3ExteZ",
        "outputId": "1af96286-7b5c-4084-fcc3-8fb422256c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train (47552, 1001)\n",
            "y_train (47552,)\n",
            "X_test (11888, 1001)\n",
            "y_test (11888,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_(y_proba,y_test):\n",
        "  proba = y_proba[:,1] >= 0.3\n",
        "  proba = proba.astype(np.int) \n",
        "  return f1_score( proba,y_test) "
      ],
      "metadata": {
        "id": "2HYOODQ4yUik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=[3]\n",
        "accuracy_train=[]\n",
        "accuracy_test=[]\n",
        "metadata = {}\n",
        "\n",
        "for i in tqdm(k):\n",
        "    model=KNeighborsClassifier(n_neighbors=i)\n",
        "    model.fit(X_train,y_train)\n",
        "    y_pred=model.predict(X_train)\n",
        "    acc=accuracy_score(y_pred,y_train)\n",
        "\n",
        "    print('for k=',i,'Accuracy Score',acc)\n",
        "\n",
        "    accuracy_train.append(acc)\n",
        "    y_proba=model.predict_proba(X_train)\n",
        "    f1_scor_train=f1_score_(y_proba,y_train)\n",
        "\n",
        "    print('for k=',i,'f1 score ',f1_scor_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwo1lbWoyZrQ",
        "outputId": "f1c56fac-ff10-463b-de2f-8a275e0e2986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for k= 3 Accuracy Score 0.916239064602961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "100%|██████████| 1/1 [05:45<00:00, 345.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for k= 3 f1 score  0.8518558326167956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if X_test is not None and y_test is not None:\n",
        "    for i in tqdm(k):\n",
        "        model=KNeighborsClassifier(n_neighbors=i)\n",
        "        model.fit(X_train,y_train)\n",
        "        y_pred=model.predict(X_test)\n",
        "        acc=accuracy_score(y_pred,y_test)\n",
        "\n",
        "        print('for k=',i,'Accuracy Score',acc)\n",
        "\n",
        "        accuracy_test.append(acc)\n",
        "        y_proba=model.predict_proba(X_test)\n",
        "        f1_scor_test=f1_score_(y_proba,y_test)\n",
        "\n",
        "        print('for k=',i,'f1 score ',f1_scor_test)\n",
        "\n",
        "metadata[\"accuracy_train\"] = accuracy_train\n",
        "metadata[\"accuracy_test\"] = accuracy_test\n",
        "metadata[\"f1_scor_train\"] = f1_scor_train\n",
        "metadata[\"f1_scor_test\"] = f1_scor_test\n",
        "metadata[\"y_pred\"] = y_pred\n"
      ],
      "metadata": {
        "id": "FGN9arjJyosY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee7e178-4fe5-4086-96b8-da361f50d4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for k= 3 Accuracy Score 0.8595222072678331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "100%|██████████| 1/1 [01:23<00:00, 83.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for k= 3 f1 score  0.8238648216396635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Deploy model "
      ],
      "metadata": {
        "id": "yDBGpZuYy-hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlfoundry\n",
        "run = mlfoundry.get_client().create_run(project_name=\"racial-slur-detector\", run_name=f\"train-{datetime.now().strftime('%m-%d-%Y')}\")\n",
        "run.log_params(model.get_params())\n",
        "\n",
        "with open('vectorizer.pickle', 'wb') as fout:\n",
        "    pickle.dump((bow), fout)\n",
        "    \n",
        "run.log_metrics({\n",
        "    'train/accuracy_score': metadata[\"accuracy_train\"][-1],\n",
        "    'train/f1': metadata[\"f1_scor_train\"],\n",
        "    'test/accuracy_score': metadata[\"accuracy_test\"][-1],\n",
        "    'test/f1': metadata[\"f1_scor_test\"]})\n",
        "\n",
        "# run.log_dataset(\n",
        "#     dataset_name='train',\n",
        "#     features=X_train,\n",
        "#     actuals=y_train)\n",
        "\n",
        "# run.log_dataset(\n",
        "#     dataset_name='test',\n",
        "#     features=X_test,\n",
        "#     predictions=metadata[\"y_pred\"],\n",
        "#     actuals=y_test,\n",
        "# )\n",
        "\n",
        "model_version = run.log_model(\n",
        "    name=\"KNN-classifier\",\n",
        "    model=model,\n",
        "    framework=\"sklearn\",\n",
        "    description=\"model trained for racial slur detection\",\n",
        ")\n",
        "model_artifact = run.log_artifact(local_path=\"vectorizer.pickle\", artifact_path=\"my-artifacts\")\n",
        "\n",
        "print(f\"Logged model: {model_version.fqn}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5MGP9_zuQfu",
        "outputId": "9395e0c4-7621-409c-f158-0254040f99b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[mlfoundry] 2022-11-06T18:27:07+0000 INFO Welcome user! You are logged in as Urja-Jobanputra\n",
            "Link to the dashboard for the run: https://app.truefoundry.com/mlfoundry/365/run/9401703fcedb43ff80580a43d6256c37/\n",
            "[mlfoundry] 2022-11-06T18:27:08+0000 WARNING failed to log git info because git repository is not present\n",
            "[mlfoundry] 2022-11-06T18:27:08+0000 INFO Run 'truefoundry/Urja-Jobanputra/twitter-sentiment/train-11-06-2022-20' has started.\n",
            "[mlfoundry] 2022-11-06T18:27:09+0000 INFO Parameters logged successfully\n",
            "[mlfoundry] 2022-11-06T18:27:09+0000 INFO Metrics logged successfully\n",
            "[mlfoundry] 2022-11-06T18:27:09+0000 INFO Logging model and additional files, this might take a while ...\n",
            "[mlfoundry] 2022-11-06T18:27:09+0000 INFO Serializing model files to model version contents\n",
            "[mlfoundry] 2022-11-06T18:27:15+0000 INFO Packaging and uploading files to remote ...\n",
            "[mlfoundry] 2022-11-06T18:27:34+0000 INFO Logged model successfully with fqn 'model:truefoundry/Urja-Jobanputra/twitter-sentiment/KNN-classifier:7'\n",
            "[mlfoundry] 2022-11-06T18:27:34+0000 INFO Logging 'vectorizer.pickle' file as artifact to 'my-artifacts/vectorizer.pickle', this might take a while ...\n",
            "Logged model: model:truefoundry/Urja-Jobanputra/twitter-sentiment/KNN-classifier:7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating FastAPI inference endpoint "
      ],
      "metadata": {
        "id": "VpD1r5WFD9EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import mlfoundry as mlf\n",
        "import pandas as pd\n",
        "import yaml\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "app = FastAPI(docs_url=\"/\")\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to Racial Slur Detector inference\"}\n",
        "\n",
        "class SentimentAnalysis(BaseModel):\n",
        "    tweet: str\n",
        " \n",
        "with open(\"infer.yaml\", \"r\") as stream:\n",
        "    try:\n",
        "        env_vars = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "# Load the model from MLFoundry by proving the MODEL_FQN\n",
        "client = mlf.get_client(api_key=env_vars['components'][0]['env']['MLF_API_KEY'],tracking_uri=env_vars['components'][0]['env']['MLF_HOST'])\n",
        "model_version = client.get_model(env_vars['components'][0]['env']['MODEL_FQN'])\n",
        "model = model_version.load()\n",
        "\n",
        "def preprocessed_tweet(test):\n",
        "    text = test['tweet'].values.tolist()\n",
        "\n",
        "    stopword=nltk.corpus.stopwords.words('english')\n",
        "    stopword.remove('not')\n",
        "    for index,text_ in enumerate(text):\n",
        "        text_=re.sub(r'@[\\w]*','',text_) #Removing Twitter Handles (@user)\n",
        "        text_=re.sub(r'http/S+','',text_) #Removing urls from text \n",
        "        text_=re.sub(r'[^A-Za-z#]',' ',text_) #Removing Punctuations, Numbers, and Special Characters\n",
        "        text_=\" \".join(i.lower() for i in text_.split() if i.lower() not in stopword) #Removing stopword\n",
        "        text[index]=text_\n",
        "\n",
        "    #Stemming the word\n",
        "    pt=PorterStemmer()\n",
        "    wordnet=WordNetLemmatizer()\n",
        "    for index,text_ in enumerate(text):\n",
        "        text_=\" \".join(pt.stem(i) for i in text_.split())\n",
        "        text_=\" \".join(wordnet.lemmatize(i) for i in text_.split())  \n",
        "        text[index]=text_\n",
        "\n",
        "    df_test = pd.DataFrame()\n",
        "    df_test['preprocess_tweet'] = text\n",
        "\n",
        "    run = client.get_run(env_vars['components'][0]['env']['RUN_ID'])\n",
        "\n",
        "    vectorizer_path = run.download_artifact(path=\"my-artifacts/vectorizer.pickle\")\n",
        "    with open(vectorizer_path, 'rb') as f:\n",
        "        bow = pickle.load(f)\n",
        "\n",
        "    bow_df=bow.transform(df_test['preprocess_tweet']).toarray()\n",
        "    print('feature name==',bow.get_feature_names_out()[:10])\n",
        "    print('number of uniqe words',bow_df.shape[1])\n",
        "    print('shape',bow_df.shape)\n",
        "    bow_train=pd.DataFrame(bow_df)\n",
        "    bow_train['length_tweet']=df_test['preprocess_tweet'].str.len()\n",
        "    return bow_train\n",
        "\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(tweet):\n",
        "    predict = [tweet]\n",
        "    test = pd.DataFrame(data=[predict],columns=['tweet'])\n",
        "    to_predict = preprocessed_tweet(test)\n",
        "\n",
        "    print(to_predict.shape)\n",
        "    prediction = model.predict(to_predict)\n",
        "    return {'sentiment' : prediction.tolist()[0]}\n",
        "\n",
        "out = predict(\"It's a good day\")\n",
        "print(out)"
      ],
      "metadata": {
        "id": "R1_Jfu81z88w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca4ea5f-144f-41d7-e7ec-a2e9a3c2e35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "fastapi==0.82.0\n",
        "uvicorn==0.18.3\n",
        "pydantic==1.10.2\n",
        "mlfoundry>=0.4.3,<0.5\n",
        "pandas==1.4.4\n",
        "nltk\n",
        "sklearn\n",
        "numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmbKxucmEQvb",
        "outputId": "96733bb8-199f-4c85-e12b-4593e5160116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Deploying the FastAPI endpoint as a service to ServiceFoundry\n"
      ],
      "metadata": {
        "id": "dZ2A9ephEVUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install servicefoundry\n",
        "!sfy logout\n",
        "!sfy use server https://app.truefoundry.com\n",
        "!sfy login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n72IlBXvNh6R",
        "outputId": "ba2b9c2d-4464-4069-8ec8-9d69d01d5f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting servicefoundry\n",
            "  Downloading servicefoundry-0.2.20-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.27.1\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<5.0.0,>=4.3.0\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting jsonschema<4.0.0,>=3.2.0\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting questionary<2.0.0,>=1.10.0\n",
            "  Downloading questionary-1.10.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (2.6.0)\n",
            "Collecting Mako<2.0.0,>=1.1.6\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting gitignorefile<2.0.0,>=1.1.2\n",
            "  Downloading gitignorefile-1.1.2.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging<22.0,>=21.3 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (21.3)\n",
            "Collecting rich-click<2.0.0,>=1.2.1\n",
            "  Downloading rich_click-1.5.2-py3-none-any.whl (20 kB)\n",
            "Collecting click<9.0.0,>=8.0.4\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.12.0\n",
            "  Downloading Pygments-2.13.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mistune<0.9.0,>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (0.8.4)\n",
            "Collecting prometheus-client<0.15.0,>=0.14.1\n",
            "  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting fastapi<0.79.0,>=0.78.0\n",
            "  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting python-dotenv<0.21.0,>=0.20.0\n",
            "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
            "Collecting python-socketio[client]<6.0.0,>=5.5.2\n",
            "  Downloading python_socketio-5.7.2-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting uvicorn<0.19.0,>=0.18.2\n",
            "  Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7.0,>=6.0 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (6.0)\n",
            "Requirement already satisfied: importlib-metadata<5.0,>=4.2 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (4.13.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (2.8.2)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (1.10.2)\n",
            "Collecting rich<13.0.0,>=12.0.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: GitPython<4.0.0,>=3.1.27 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (3.1.29)\n",
            "Requirement already satisfied: importlib-resources<6.0.0,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from servicefoundry) (5.10.0)\n",
            "Collecting starlette==0.19.1\n",
            "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi<0.79.0,>=0.78.0->servicefoundry) (2.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython<4.0.0,>=3.1.27->servicefoundry) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython<4.0.0,>=3.1.27->servicefoundry) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0,>=4.2->servicefoundry) (3.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0,>=3.2.0->servicefoundry) (22.1.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0,>=3.2.0->servicefoundry) (1.15.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0,>=3.2.0->servicefoundry) (0.18.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0,>=3.2.0->servicefoundry) (57.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako<2.0.0,>=1.1.6->servicefoundry) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<22.0,>=21.3->servicefoundry) (3.0.9)\n",
            "Collecting python-engineio>=4.3.0\n",
            "  Downloading python_engineio-4.3.4-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting bidict>=0.21.0\n",
            "  Downloading bidict-0.22.0-py3-none-any.whl (36 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from questionary<2.0.0,>=1.10.0->servicefoundry) (2.0.10)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary<2.0.0,>=1.10.0->servicefoundry) (0.2.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.27.1->servicefoundry) (1.25.11)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.27.1->servicefoundry) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.27.1->servicefoundry) (2022.9.24)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gitignorefile\n",
            "  Building wheel for gitignorefile (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gitignorefile: filename=gitignorefile-1.1.2-py3-none-any.whl size=6686 sha256=4876fc47387ab8e6ab9e35219db7c6f7c1066dfbe89b77dfa6137a8427b89668\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/5d/74/a0d64236ae78147af494f2f58d98035c28799786450d110c68\n",
            "Successfully built gitignorefile\n",
            "Installing collected packages: typing-extensions, sniffio, python-engineio, pygments, commonmark, bidict, anyio, websocket-client, starlette, rich, requests, python-socketio, h11, click, uvicorn, rich-click, questionary, python-dotenv, prometheus-client, Mako, jsonschema, gitignorefile, fastapi, servicefoundry\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "thinc 8.1.5 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "spacy 3.4.2 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\n",
            "confection 0.0.3 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.2.3 anyio-3.6.2 bidict-0.22.0 click-8.1.3 commonmark-0.9.1 fastapi-0.78.0 gitignorefile-1.1.2 h11-0.14.0 jsonschema-3.2.0 prometheus-client-0.14.1 pygments-2.13.0 python-dotenv-0.20.0 python-engineio-4.3.4 python-socketio-5.7.2 questionary-1.10.0 requests-2.28.1 rich-12.6.0 rich-click-1.5.2 servicefoundry-0.2.20 sniffio-1.3.0 starlette-0.19.1 typing-extensions-4.4.0 uvicorn-0.18.3 websocket-client-1.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "click",
                  "jsonschema",
                  "pygments",
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mYou are already logged out\u001b[0m\n",
            "Login Code: G64XCN\n",
            "Waiting for authentication. Go to the following url to complete the authentication: \n",
            "\u001b[4;94mhttps://app.truefoundry.com/authorize/device?\u001b[0m\u001b[4;94muserCode\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mG64XCN\u001b[0m\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://app.truefoundry.com/authorize/device?userCode=G64XCN'\n",
            "\u001b[1;32mLogin Successful!\u001b[0m\n",
            "Session stored at \u001b[35m/root/.truefoundry/\u001b[0m\u001b[95msessions.json\u001b[0m\n",
            "\u001b[33mSetting cluster \u001b[0m\u001b[33m'tfy-cluster-euwe1'\u001b[0m\u001b[33m as default\u001b[0m\n",
            "\u001b[33mYou are logged in as \u001b[0m\u001b[33m'Urja-Jobanputra'\u001b[0m\u001b[33m with email \u001b[0m\u001b[33m'urjajobanputra657@gmail.com'\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "name: sentiment-analysis-inf\n",
        "components:\n",
        "- name: sentiment-analysis-inf\n",
        "  type: job\n",
        "  image:\n",
        "    type: build\n",
        "    build_source:\n",
        "      type: local\n",
        "    build_spec:\n",
        "      type: tfy-python-buildpack\n",
        "      command: uvicorn infer_realtime:app --port 8000 --host 0.0.0.0\n",
        "  env:\n",
        "    MLF_HOST: https://app.truefoundry.com\n",
        "    MLF_API_KEY: 'djE6dHJ1ZWZvdW5kcnk6VXJqYS1Kb2JhbnB1dHJhOjUwYjQ4NQ==' # Get the API_KEY from the settings of truefoundry account\n",
        "    MODEL_FQN: <paste you model version fqn here>\n",
        "    WORKSPACE_FQN: <paste you workspace fqn here> # Add the WORKSPACE_FQN\n",
        "    RUN_ID: <paste you run id here>\n",
        "  ports:\n",
        "      - port: 8000"
      ],
      "metadata": {
        "id": "xXeohoKPRaNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from servicefoundry import Build, PythonBuild, Service, Resources\n",
        "\n",
        "with open(\"infer.yaml\", \"r\") as stream:\n",
        "    try:\n",
        "        env_vars = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "# Since we are using FastAPI service to infer the model we'll use the Service() method \n",
        "service = Service(\n",
        "    name=env_vars['name'],\n",
        "    image=Build(\n",
        "        build_spec=PythonBuild(\n",
        "            command=env_vars['components'][0]['image']['build_spec']['command'],\n",
        "        ),\n",
        "    ),\n",
        "    ports=[{\"port\": 8000}],\n",
        "    resources=Resources(memory_limit=1500, memory_request=1000),\n",
        ")\n",
        "service.deploy(workspace_fqn=env_vars['components'][0]['env']['WORKSPACE_FQN'])"
      ],
      "metadata": {
        "id": "sKIGCDWJEZsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a demo webapp "
      ],
      "metadata": {
        "id": "2qEQoZWVFmMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !mkdir -p /content/demo/\n",
        "%cd /content/demo/"
      ],
      "metadata": {
        "id": "BQS7-5cSEfZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demo.py\n",
        "from operator import concat\n",
        "import streamlit as st\n",
        "import requests\n",
        "import yaml\n",
        "import base64\n",
        "\n",
        "def add_bg_from_local(image_file):\n",
        "    with open(image_file, \"rb\") as image_file:\n",
        "        encoded_string = base64.b64encode(image_file.read())\n",
        "    st.markdown(\n",
        "    f\"\"\"\n",
        "    <style>\n",
        "    .stApp {{\n",
        "        background-image: url(data:image/{\"jpeg\"};base64,{encoded_string.decode()});\n",
        "        background-size: cover\n",
        "    }}\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "with open(\"demo.yaml\", \"r\") as stream:\n",
        "    try:\n",
        "        env_vars = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "def fetch(session, url):\n",
        "    try:\n",
        "        result = session.get(url)\n",
        "        return result.json()\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def main():\n",
        "    request_url = env_vars['components'][0]['env']['INFER_URL']\n",
        "    st.set_page_config(page_title=\"Racial Slur Detector\")\n",
        "    add_bg_from_local('image2.jpeg')\n",
        "    st.title(\"Racial Slur Detector\")\n",
        "    st.header('Welcome to Racial Slur Detector!')\n",
        "    st.write('This is a sample app that demonstrates the prowess of ServiceFoundry ML model deployment.🚀')\n",
        "    st.write('Visit the [Github](https://github.com/urja0901/racial-slur-detector) repo for code or [Google Colab](https://colab.research.google.com/drive/1mWhYBiVnduQHrUqazu-fwzWze7zjGt-W?usp=sharing) notebook for a quick start.')\n",
        "    with st.form(\"my_form\"):\n",
        "        \n",
        "        sentiment_text = st.text_input('Sentiment Text',value=\"It's a good day!\")\n",
        "\n",
        "        features = {\n",
        "                \"tweet\": sentiment_text\n",
        "            }\n",
        "            \n",
        "        submitted = st.form_submit_button(\"Submit\")\n",
        "        if submitted:\n",
        "            data = requests.post(url=concat(request_url, \"/predict\"), params=features).json()\n",
        "            if data:\n",
        "                if data[\"sentiment\"] == 0 :\n",
        "                    return_val = \"positive\"\n",
        "                else : \n",
        "                    return_val = \"negative (racist/sexist)\"\n",
        "                st.metric(label=\"sentiment\",value=data['sentiment'])\n",
        "            else:\n",
        "                st.error(\"Error\")\n",
        "\n",
        "    st.image('image1.jpeg', use_column_width='always')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Fio2l24XEug9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "pandas==1.4.4\n",
        "\n",
        "# for experiment tracking and model registry\n",
        "mlfoundry>=0.4.3,<0.5\n",
        "\n",
        "# for deploying our job deployments\n",
        "servicefoundry>=0.1.97,<0.2.0\n",
        "\n",
        "streamlit>=1.13.0"
      ],
      "metadata": {
        "id": "zhyYyXaDE8Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "\n",
        "from servicefoundry import Build, PythonBuild, Resources, Service\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# parsing the input arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--workspace_fqn\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"fqn of workspace where you want to deploy\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--inference_server_url\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"end point of the trained model that would be used for inference\",\n",
        ")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "# creating a service object and defining all the configurations\n",
        "service = Service(\n",
        "    name=\"sentiment-analysis-demo\",\n",
        "    image=Build(\n",
        "        build_spec=PythonBuild(\n",
        "            command=\"streamlit run demo.py\",\n",
        "            python_version=\"3.9\",\n",
        "        ),\n",
        "    ),\n",
        "    env={\n",
        "        # These will automatically map the secret value to the environment variable.\n",
        "        \"MLF_HOST\": \"https://app.truefoundry.com\",\n",
        "        \"INFERENCE_SERVER_URL\": args.inference_server_url,\n",
        "        \"MLF_API_KEY\": \"djE6dHJ1ZWZvdW5kcnk6VXJqYS1Kb2JhbnB1dHJhOjUwYjQ4NQ==\",\n",
        "    },\n",
        "    ports=[{\"port\": 8501}], #In public cloud deployment TrueFoundry exposes port 8501\n",
        "    resources=Resources(\n",
        "        cpu_request=0.5, cpu_limit=.5, memory_limit=2500, memory_request=1500\n",
        "    ),\n",
        ")\n",
        "service.deploy(workspace_fqn=args.workspace_fqn)"
      ],
      "metadata": {
        "id": "HqW9TODuFD6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0b-0j9a8LKAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}